<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>squirrelfriends.squirrel API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>squirrelfriends.squirrel</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from ._lightgbm import lgbSquirrel
from ._grid_search import search_cv
from ._sampling import smote_sampling

__all__ = [
    &#34;lgbSquirrel&#34;,
    &#34;search_cv&#34;,
    &#34;smote_sampling&#34;
]</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="squirrelfriends.squirrel.search_cv"><code class="name flex">
<span>def <span class="ident">search_cv</span></span>(<span>model, grid, X, y, eval_func=None, n_splits=4, seed=2020)</span>
</code></dt>
<dd>
<div class="desc"><p>Grid search.</p>
<h2 id="args">Args</h2>
<dl>
<dt>model (:obj:): model to search hyper-parameter.</dt>
<dt><strong><code>grid</code></strong> :&ensp;<code>dict</code></dt>
<dd>(param name: list of value).</dd>
<dt><strong><code>X</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>training data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>test data.</dd>
<dt><strong><code>eval_func</code></strong> :&ensp;<code>func</code></dt>
<dd>callable function in evaluation,
eval_func(y_true, y_pred).</dd>
<dt><strong><code>n_splits</code></strong> :&ensp;<code>int</code></dt>
<dd>number of splits in Kflod().</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>random seed in Kfold().</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>search_results (dict): result og grid search,</code></dt>
<dd>
<ul>
<li>parameter (dict): param dictionary,</li>
<li>cv_results (dict): dict of cv results,</li>
<li>eval_score (float) mean of evalation score.</li>
</ul>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def search_cv(model, grid, X, y, eval_func=None, n_splits=4, seed=2020):
    &#34;&#34;&#34;Grid search.

    Args:
        model (:obj:): model to search hyper-parameter.
        grid (dict): (param name: list of value).
        X (DataFrame): training data.
        y (DataFrame): test data.
        eval_func (func): callable function in evaluation,
            eval_func(y_true, y_pred).
        n_splits (int): number of splits in Kflod().
        seed (int): random seed in Kfold().

    Returns:
        search_results (dict): result og grid search,
            - parameter (dict): param dictionary,
            - cv_results (dict): dict of cv results,
            - eval_score (float) mean of evalation score.
    &#34;&#34;&#34;

    params = _parse_param_grid(grid)
    search_results = []

    for param in params:
        logging.info(&#34;param: &#34; + str(param))

        # Search cv on lightgbm
        if hasattr(model, &#34;__model__&#34;) and model.__model__ == &#34;lgbSquirrel&#34;:
            model.eval_func = eval_func
            if &#34;num_iterations&#34; in param.keys():
                model.build_lgb(param)
            else:
                model.build_sklgb(param)
            cv_results = model.kfold_cv_lgb_train(
                X=X, y=y, n_splits=n_splits, seed=seed)

        # Search cv on other sklearn model
        else:
            cv_model = model(**param)

            kfold = KFold(n_splits=n_splits, shuffle=True, random_state=seed)

            cv_results = []

            for fold, (ind_train, ind_test) in enumerate(kfold.split(X, y)):
                logging.info(&#34;cv: %d out of %d&#34; % (fold + 1, n_splits))

                X_train, X_test = X.loc[ind_train, :], X.loc[ind_test, :]
                y_train, y_test = y[ind_train], y[ind_test]

                cv_model.fit(X_train, y_train)

                val_pre = cv_model.predict(X_test)
                eval_score = eval_func(y_test, val_pre)

                if hasattr(cv_model, &#34;feature_importances_&#34;):
                    importances = cv_model.feature_importances_
                else:
                    importances = None

                model_result = {
                    &#34;model&#34;: model,
                    &#34;importances&#34;: importances,
                    &#34;eval_score&#34;: eval_score
                }
                cv_results.append(model_result)

        eval_scores = [result[&#34;eval_score&#34;] for result in cv_results]

        if None not in eval_scores:
            logging.info(&#34;average score is %f, std is %f&#34; %
                         (np.mean(eval_scores), np.std(eval_scores)))

        one_search = {
            &#34;parameter&#34;: param,
            &#34;cv_results&#34;: cv_results,
            &#34;eval_score&#34;: np.mean(eval_scores)
        }
        search_results.append(one_search)

    return search_results</code></pre>
</details>
</dd>
<dt id="squirrelfriends.squirrel.smote_sampling"><code class="name flex">
<span>def <span class="ident">smote_sampling</span></span>(<span>data, features, label, k=5, classes={'maj': 0, 'min': 1}, percentages={'over': 9, 'under': 0.5})</span>
</code></dt>
<dd>
<div class="desc"><p>Implement smote sampling.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>training data.</dd>
<dt>features(list of str): column names of feature to be used in knn.</dt>
<dt><strong><code>label</code></strong> :&ensp;<code>str</code></dt>
<dd>column name of.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code></dt>
<dd><code>k</code>-nn.</dd>
<dt><strong><code>classes</code></strong> :&ensp;<code>dict</code></dt>
<dd>major and minor classes.</dd>
<dt><strong><code>percentages</code></strong> :&ensp;<code>dict</code></dt>
<dd>the fraction of up and under sampling.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>result (DataFrame): result of smote sampling.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def smote_sampling(data, features, label, k=5,
                   classes={&#34;maj&#34;: 0, &#34;min&#34;: 1},
                   percentages={&#34;over&#34;: 9, &#34;under&#34;: 0.5}):
    &#34;&#34;&#34;Implement smote sampling.

    Args:
      data (DataFrame): training data.
      features(list of str): column names of feature to be used in knn.
      label (str): column name of.
      k (int): `k`-nn.
      classes (dict): major and minor classes.
      percentages (dict): the fraction of up and under sampling.

    Returns:
      result (DataFrame): result of smote sampling.
    &#34;&#34;&#34;

    random.seed(100)

    if percentages[&#34;under&#34;] &gt; 1 or percentages[&#34;under&#34;] &lt; 0.1:
        raise ValueError(&#34;Percentage Under must be in range 0.1 - 1&#34;)
    if percentages[&#34;over&#34;] &lt; 1:
        raise ValueError(&#34;Percentage Over must be in at least 1&#34;)

    data = data[[label, *features]].copy()

    data_min = data.loc[data[label] == classes[&#34;min&#34;]].reset_index()
    data_maj = data.loc[data[label] == classes[&#34;maj&#34;]].reset_index()

    # Train knn
    data_min_features = data_min[features]
    nbrs = (neighbors.NearestNeighbors(n_neighbors=k, algorithm=&#34;auto&#34;)
                     .fit(data_min_features))
    neighbours = nbrs.kneighbors(data_min_features)[1]
    # Upsampling by knn
    new_rows = []
    for i in range(len(data_min_features)):
        for _ in range(ceil(percentages[&#34;over&#34;] - 1)):
            chance = 1 - percentages[&#34;over&#34;] / ceil(percentages[&#34;over&#34;])
            if random.uniform(0, 1) &gt;= chance:
                # Randomly pick a neighbour.
                neigh = neighbours[i][random.randint(0, k - 1)]
                diff = data_min_features.loc[neigh] - data_min_features.loc[i]
                new_rec = data_min_features.loc[i] + random.random() * diff
                new_rows.append(new_rec.to_dict())
    new_data_min = pd.DataFrame(new_rows)
    new_data_min[label] = classes[&#34;min&#34;]

    # Downsampling
    new_data_maj = data_maj.sample(frac=float(percentages[&#34;under&#34;]),
                                   replace=False, random_state=1024)

    frames = [data_min, new_data_min, new_data_maj]

    result = pd.concat(frames).reset_index()[[label, *features]]

    return result[[*features, label]]</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="squirrelfriends.squirrel.lgbSquirrel"><code class="flex name class">
<span>class <span class="ident">lgbSquirrel</span></span>
<span>(</span><span>features=None, cat_features=None, task='classification', eval_func=None, early_stopping_rounds=50, verbose_eval=10)</span>
</code></dt>
<dd>
<div class="desc"><p>Class to implement lightgbm by using either lightgbm or sklearn API.
It is composed with two parts:
1. build the model.
2. train the model.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt>features(list of str): column names of feature.</dt>
<dt>cat_features(list of str): categorical features,</dt>
<dt>specified in lgb.Dataset.</dt>
<dt><strong><code>task</code></strong> :&ensp;<code>str</code></dt>
<dd>{"regression",
"classification"},
type of machine learning task.</dd>
<dt><strong><code>eval_func</code></strong> :&ensp;<code>func</code></dt>
<dd>callable function in evaluation,
eval_func(y_true, y_pred).</dd>
<dt><strong><code>early_stopping_rounds</code></strong> :&ensp;<code>int</code></dt>
<dd>early stopping rounds.</dd>
<dt><strong><code>verbose_eval</code></strong> :&ensp;<code>int</code></dt>
<dd>log evaluation result per <code>verbose_eval</code> round.</dd>
<dt><strong><code>sk_api</code></strong> :&ensp;<code>boolean</code></dt>
<dd>if using sklearn API.</dd>
<dt><strong><code>feature_importances_</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>feature importance table.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class lgbSquirrel(object):
    &#34;&#34;&#34;Class to implement lightgbm by using either lightgbm or sklearn API.
    It is composed with two parts:
        1. build the model.
        2. train the model.

    Attributes:
        features(list of str): column names of feature.
        cat_features(list of str): categorical features,
            specified in lgb.Dataset.
        task (str): {&#34;regression&#34;,  &#34;classification&#34;},
            type of machine learning task.
        eval_func (func): callable function in evaluation,
            eval_func(y_true, y_pred).
        early_stopping_rounds (int): early stopping rounds.
        verbose_eval (int): log evaluation result per `verbose_eval` round.
        sk_api (boolean): if using sklearn API.
        feature_importances_ (DataFrame): feature importance table.
    &#34;&#34;&#34;

    def __init__(self, features=None, cat_features=None,
                 task=&#34;classification&#34;, eval_func=None,
                 early_stopping_rounds=50, verbose_eval=10):
        self.__model__ = &#34;lgbSquirrel&#34;
        self.features = features
        self.cat_features = cat_features
        self.task = task
        self.eval_func = eval_func
        self.early_stopping_rounds = early_stopping_rounds
        self.verbose_eval = verbose_eval
        self.sk_api = None
        self.feature_importances_ = None

    def build_sklgb(self, trn_params):
        &#34;&#34;&#34;Parse parameters in to build the sklearn lgb.
        http://lightgbm.readthedocs.io/en/latest/Parameters.html

        Args:
            trn_params (dict): params to build model obj.
            boosting_type : string, optional (default=&#34;gbdt&#34;)
                &#34;gbdt&#34;, traditional Gradient Boosting Decision Tree.
                &#34;dart&#34;, Dropouts meet Multiple Additive Regression Trees.
                &#34;goss&#34;, Gradient-based One-Side Sampling.
                &#34;rf&#34;, Random Forest.
            num_leaves : int, optional (default=31)
                Maximum tree leaves for base learners.
            max_depth : int, optional (default=-1)
                Maximum tree depth for base learners, &lt;=0 means no limit.
            learning_rate : float, optional (default=0.1)
                Boosting learning rate.
                You can use `callbacks` parameter of `fit` method
                    to shrink/adapt learning rate
                    in training using `reset_parameter` callback.
                Note, that this will ignore the `learning_rate`
                    argument in training.
            n_estimators : int, optional (default=100)
                Number of boosted trees to fit.
            subsample_for_bin : int, optional (default=200000)
                Number of samples for constructing bins.
            objective : string, callable or None, optional (default=None)
                Specify the learning task and the corresponding
                    learning objective or a custom objective function
                    to be used (see note below).
                Default: &#34;regression&#34; for LGBMRegressor, &#34;binary&#34; or
                    &#34;multiclass&#34; for LGBMClassifier,
                    &#34;lambdarank&#34; for LGBMRanker.
            class_weight : dict, &#34;balanced&#34; or None, optional (default=None)
                Weights associated with classes
                    in the form `{class_label: weight}`.
                Use this parameter only for multi-class classification task;
                    for binary classification task you may use `is_unbalance`
                    or `scale_pos_weight` parameters.
                Note, that the usage of all these parameters will result in
                    poor estimates of the individual class probabilities.
                You may want to consider performing probability calibration
                (https://scikit-learn.org/stable/modules/calibration.html)
                    of your model.
                The &#34;balanced&#34; mode uses the values of y
                    to automatically adjust weights
                    inversely proportional to class frequencies
                    in the input data
                    as `n_samples / (n_classes * np.bincount(y))`.
                If None, all classes are supposed to have weight one.
                Note, that these weights will be multiplied with
                    `sample_weight` (passed through the `fit` method)
                    if `sample_weight` is specified.
            min_split_gain : float, optional (default=0.)
                Minimum loss reduction required to make a further partition on
                    a leaf node of the tree.
            min_child_weight : float, optional (default=1e-3)
                Minimum sum of instance weight (hessian)
                needed in a child (leaf).
            min_child_samples : int, optional (default=20)
                Minimum number of data needed in a child (leaf).
            subsample : float, optional (default=1.)
                Subsample ratio of the training instance.
            subsample_freq : int, optional (default=0)
                Frequence of subsample, &lt;=0 means no enable.
            colsample_bytree : float, optional (default=1.)
                Subsample ratio of columns when constructing each tree.
            reg_alpha : float, optional (default=0.)
                L1 regularization term on weights.
            reg_lambda : float, optional (default=0.)
                L2 regularization term on weights.
            random_state : int, RandomState object or None,
                optional (default=None) Random number seed.
                If int, this number is used to seed the C++ code.
                If RandomState object (numpy), a random integer
                    is picked based on its state to seed the C++ code.
                If None, default seeds in C++ code are used.
            n_jobs : int, optional (default=-1)
                Number of parallel threads.
            silent : bool, optional (default=True)
                Whether to print messages while running boosting.
            importance_type : string, optional (default=&#34;split&#34;)
                The type of feature importance to be filled into
                    `feature_importances_`.
                If &#34;split&#34;, result contains numbers of times the feature
                    is used in a model.
                If &#34;gain&#34;, result contains total gains of splits
                    which use the feature.
        &#34;&#34;&#34;

        self.sk_api = True
        self.trn_params = trn_params

    def build_lgb(self, trn_params,
                  dtrain_params=None, dtest_params=None,
                  weight=None):
        &#34;&#34;&#34;Parse parameters in to build lgb.
        https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst

        Args:
            trn_params (dict): params parsed into lgb.train().
                num_iterations will be parsed as `lgb_num_iterations`.
                lgb_num_iterations (int): number of boosting iterations.
            dtrain_params (dict): params of lgb.Dataset() for training data.
            dtest_params (dict): params of lgb.Dataset() for testing data,
                if None, directly use `dtrain_params`.
            weight (array-like): weight for each instance.
        &#34;&#34;&#34;

        self.sk_api = False
        self.trn_params = trn_params
        self.dtrain_params = dtrain_params
        self.dtest_params = dtest_params if dtest_params else dtrain_params
        self.weight = weight
        if &#34;num_iterations&#34; in self.trn_params.keys():
            self.lgb_num_iterations = self.trn_params[&#34;num_iterations&#34;]
            self.trn_params.pop(&#34;num_iterations&#34;)
        else:
            self.lgb_num_iterations = 1000

    def _train_sklgb(self, data):
        &#34;&#34;&#34;Train the model using sklearn lightgbm API.

        Args:
            data ((tuple of DataFrame)): training and testing data.

        Returns:
            model_result (dict): result of trained model.
                - model (:obj:): trained model,
                - importances (DataFrame): feature importance table,
                - eval_score (float): score of model using `eval_func`.
        &#34;&#34;&#34;

        logging.info(&#34;using lightgbm sklearn api...&#34;)

        X_train, X_test, y_train, y_test = data
        X_train, X_test = X_train[self.features], X_test[self.features]

        if self.task == &#34;regression&#34;:
            model = LGBMRegressor(**self.trn_params, n_jobs=-1)
        elif self.task == &#34;classification&#34;:
            model = LGBMClassifier(**self.trn_params, n_jobs=-1)

        # Train model
        model.fit(X_train, y_train,
                  feature_name=self.features,
                  categorical_feature=self.cat_features,
                  eval_set=[(X_train, y_train), (X_test, y_test)],
                  early_stopping_rounds=self.early_stopping_rounds,
                  verbose=self.verbose_eval)

        # Calculate the feature importances
        importances = pd.DataFrame()
        importances[&#34;feature&#34;] = X_train.columns.values.tolist()
        importances[&#34;importance&#34;] = model.feature_importances_

        # Calculate eval_func function eval_score
        if self.eval_func:
            if self.task == &#34;classification&#34;:
                # Return the predicted probability
                # for each class for each sample.
                y_test_pred = model.predict_proba(X_test)
            else:
                y_test_pred = model.predict(X_test)

            eval_score = self.eval_func(y_test, y_test_pred)
            logging.info(&#34;score on validation is %f&#34; % eval_score)
        else:
            eval_score = None

        model_result = {
            &#34;model&#34;: model,
            &#34;importances&#34;: importances,
            &#34;eval_score&#34;: eval_score
        }
        return model_result

    def _train_lgb(self, data):
        &#34;&#34;&#34;Train the model using lightgbm.

        Args:
            data ((tuple of DataFrame)): training and testing data.

        Returns:
            model_result (dict): result of trained model.
                - model (:obj:): trained model,
                - importances (DataFrame): feature importance table,
                - eval_score (float): score of model using `eval_func`.
        &#34;&#34;&#34;

        logging.info(&#34;using lightgbm api...&#34;)

        X_train, X_test, y_train, y_test = data
        X_train, X_test = X_train[self.features], X_test[self.features]

        # Prepare lgb.Dataset
        d_train = lgb.Dataset(X_train, y_train,
                              feature_name=self.features,
                              categorical_feature=self.cat_features,
                              params=self.dtrain_params,
                              weight=self.weight)
        d_test = lgb.Dataset(X_test, y_test,
                             feature_name=self.features,
                             categorical_feature=self.cat_features,
                             params=self.dtest_params,
                             weight=self.weight)

        # Train model
        model = lgb.train(self.trn_params, d_train,
                          num_boost_round=self.lgb_num_iterations,
                          feature_name=self.features,
                          categorical_feature=self.cat_features,
                          valid_sets=[d_train, d_test],
                          early_stopping_rounds=self.early_stopping_rounds,
                          verbose_eval=self.verbose_eval)

        # Calculate the feature importances
        importances = pd.DataFrame()
        importances[&#34;feature&#34;] = X_train.columns.values.tolist()
        importances[&#34;importance&#34;] = model.feature_importance()

        # Calculate eval_func function eval_score
        if self.eval_func:
            y_test_pred = model.predict(X_test)  # ,
            # num_iteration=model.best_iteration)
            eval_score = self.eval_func(y_test, y_test_pred)
            logging.info(&#34;score on validation is %f&#34; % eval_score)
        else:
            eval_score = None

        model_result = {
            &#34;model&#34;: model,
            &#34;importances&#34;: importances,
            &#34;eval_score&#34;: eval_score
        }
        return model_result

    def train(self, data):
        &#34;&#34;&#34;Train the model.

        Args:
            data ((tuple of DataFrame)): training and testing data.

        Returns:
            model_result (dict): result of trained model.
                - model (:obj:): trained model,
                - importances (DataFrame): feature importance table,
                - eval_score (float): score of model using `eval_func`.
        &#34;&#34;&#34;

        if self.sk_api is None:
            raise Exception(&#34;Please call build function first.&#34;)

        if self.sk_api:
            model_result = self._train_sklgb(data)
        else:
            model_result = self._train_lgb(data)

        self.feature_importances_ = model_result[&#34;importances&#34;]

        return model_result

    def kfold_cv_lgb_train(self, X, y, n_splits, seed):
        &#34;&#34;&#34;Train lightGBM by Kfold cross validation.

        Args:
            X (DataFrame): training data.
            y (DataFrame): test data.
            n_splits (int): number of splits in Kflod().
            seed (int): random seed in Kfold().

        Returns:
            models (list of :obj:): list of models for each cv.
            importances_tables (list of DataFrame):
                list of feature importance tables for each cv.
            eval_scores (list of float): list of score for each cv
        &#34;&#34;&#34;

        if self.sk_api is None:
            raise Exception(&#34;Please call build function first.&#34;)

        kfold = KFold(n_splits=n_splits, shuffle=True, random_state=seed)

        cv_results = []

        for fold, (ind_train, ind_test) in enumerate(kfold.split(X, y)):
            logging.info(&#34;cv: %d out of %d&#34; % (fold + 1, n_splits))

            X_train, X_test = X.loc[ind_train, :], X.loc[ind_test, :]
            y_train, y_test = y[ind_train], y[ind_test]
            data = (X_train, X_test, y_train, y_test)

            cv_results.append(self.train(data=data))

        eval_scores = [result[&#34;eval_score&#34;] for result in cv_results]

        if None not in eval_scores:
            logging.info(&#34;average score is %f, std is %f&#34; %
                         (np.mean(eval_scores), np.std(eval_scores)))

        importances_lst = [result[&#34;importances&#34;] for result in cv_results]
        self.feature_importances_ = compute_cv_feature_importances(
            *importances_lst)

        return cv_results</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="squirrelfriends.squirrel.lgbSquirrel.build_lgb"><code class="name flex">
<span>def <span class="ident">build_lgb</span></span>(<span>self, trn_params, dtrain_params=None, dtest_params=None, weight=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Parse parameters in to build lgb.
<a href="https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst">https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>trn_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>params parsed into lgb.train().
num_iterations will be parsed as <code>lgb_num_iterations</code>.
lgb_num_iterations (int): number of boosting iterations.</dd>
<dt><strong><code>dtrain_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>params of lgb.Dataset() for training data.</dd>
<dt><strong><code>dtest_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>params of lgb.Dataset() for testing data,
if None, directly use <code>dtrain_params</code>.</dd>
</dl>
<p>weight (array-like): weight for each instance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_lgb(self, trn_params,
              dtrain_params=None, dtest_params=None,
              weight=None):
    &#34;&#34;&#34;Parse parameters in to build lgb.
    https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst

    Args:
        trn_params (dict): params parsed into lgb.train().
            num_iterations will be parsed as `lgb_num_iterations`.
            lgb_num_iterations (int): number of boosting iterations.
        dtrain_params (dict): params of lgb.Dataset() for training data.
        dtest_params (dict): params of lgb.Dataset() for testing data,
            if None, directly use `dtrain_params`.
        weight (array-like): weight for each instance.
    &#34;&#34;&#34;

    self.sk_api = False
    self.trn_params = trn_params
    self.dtrain_params = dtrain_params
    self.dtest_params = dtest_params if dtest_params else dtrain_params
    self.weight = weight
    if &#34;num_iterations&#34; in self.trn_params.keys():
        self.lgb_num_iterations = self.trn_params[&#34;num_iterations&#34;]
        self.trn_params.pop(&#34;num_iterations&#34;)
    else:
        self.lgb_num_iterations = 1000</code></pre>
</details>
</dd>
<dt id="squirrelfriends.squirrel.lgbSquirrel.build_sklgb"><code class="name flex">
<span>def <span class="ident">build_sklgb</span></span>(<span>self, trn_params)</span>
</code></dt>
<dd>
<div class="desc"><p>Parse parameters in to build the sklearn lgb.
<a href="http://lightgbm.readthedocs.io/en/latest/Parameters.html">http://lightgbm.readthedocs.io/en/latest/Parameters.html</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>trn_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>params to build model obj.</dd>
<dt><strong><code>boosting_type</code></strong> :&ensp;<code>string</code>, optional <code>(default="gbdt")</code></dt>
<dd>"gbdt", traditional Gradient Boosting Decision Tree.
"dart", Dropouts meet Multiple Additive Regression Trees.
"goss", Gradient-based One-Side Sampling.
"rf", Random Forest.</dd>
<dt><strong><code>num_leaves</code></strong> :&ensp;<code>int</code>, optional <code>(default=31)</code></dt>
<dd>Maximum tree leaves for base learners.</dd>
<dt><strong><code>max_depth</code></strong> :&ensp;<code>int</code>, optional <code>(default=-1)</code></dt>
<dd>Maximum tree depth for base learners, &lt;=0 means no limit.</dd>
<dt><strong><code>learning_rate</code></strong> :&ensp;<code>float</code>, optional <code>(default=0.1)</code></dt>
<dd>Boosting learning rate.
You can use <code>callbacks</code> parameter of <code>fit</code> method
to shrink/adapt learning rate
in training using <code>reset_parameter</code> callback.
Note, that this will ignore the <code>learning_rate</code>
argument in training.</dd>
<dt><strong><code>n_estimators</code></strong> :&ensp;<code>int</code>, optional <code>(default=100)</code></dt>
<dd>Number of boosted trees to fit.</dd>
<dt><strong><code>subsample_for_bin</code></strong> :&ensp;<code>int</code>, optional <code>(default=200000)</code></dt>
<dd>Number of samples for constructing bins.</dd>
<dt><strong><code>objective</code></strong> :&ensp;<code>string, callable</code> or <code>None</code>, optional <code>(default=None)</code></dt>
<dd>Specify the learning task and the corresponding
learning objective or a custom objective function
to be used (see note below).
Default: "regression" for LGBMRegressor, "binary" or
"multiclass" for LGBMClassifier,
"lambdarank" for LGBMRanker.</dd>
<dt><strong><code>class_weight</code></strong> :&ensp;<code>dict, "balanced"</code> or <code>None</code>, optional <code>(default=None)</code></dt>
<dd>Weights associated with classes
in the form <code>{class_label: weight}</code>.
Use this parameter only for multi-class classification task;
for binary classification task you may use <code>is_unbalance</code>
or <code>scale_pos_weight</code> parameters.
Note, that the usage of all these parameters will result in
poor estimates of the individual class probabilities.
You may want to consider performing probability calibration
(<a href="https://scikit-learn.org/stable/modules/calibration.html">https://scikit-learn.org/stable/modules/calibration.html</a>)
of your model.
The "balanced" mode uses the values of y
to automatically adjust weights
inversely proportional to class frequencies
in the input data
as <code>n_samples / (n_classes * np.bincount(y))</code>.
If None, all classes are supposed to have weight one.
Note, that these weights will be multiplied with
<code>sample_weight</code> (passed through the <code>fit</code> method)
if <code>sample_weight</code> is specified.</dd>
<dt><strong><code>min_split_gain</code></strong> :&ensp;<code>float</code>, optional <code>(default=0.)</code></dt>
<dd>Minimum loss reduction required to make a further partition on
a leaf node of the tree.</dd>
<dt><strong><code>min_child_weight</code></strong> :&ensp;<code>float</code>, optional <code>(default=1e-3)</code></dt>
<dd>Minimum sum of instance weight (hessian)
needed in a child (leaf).</dd>
<dt><strong><code>min_child_samples</code></strong> :&ensp;<code>int</code>, optional <code>(default=20)</code></dt>
<dd>Minimum number of data needed in a child (leaf).</dd>
<dt><strong><code>subsample</code></strong> :&ensp;<code>float</code>, optional <code>(default=1.)</code></dt>
<dd>Subsample ratio of the training instance.</dd>
<dt><strong><code>subsample_freq</code></strong> :&ensp;<code>int</code>, optional <code>(default=0)</code></dt>
<dd>Frequence of subsample, &lt;=0 means no enable.</dd>
<dt><strong><code>colsample_bytree</code></strong> :&ensp;<code>float</code>, optional <code>(default=1.)</code></dt>
<dd>Subsample ratio of columns when constructing each tree.</dd>
<dt><strong><code>reg_alpha</code></strong> :&ensp;<code>float</code>, optional <code>(default=0.)</code></dt>
<dd>L1 regularization term on weights.</dd>
<dt><strong><code>reg_lambda</code></strong> :&ensp;<code>float</code>, optional <code>(default=0.)</code></dt>
<dd>L2 regularization term on weights.</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int, RandomState object</code> or <code>None,</code></dt>
<dd>optional (default=None) Random number seed.
If int, this number is used to seed the C++ code.
If RandomState object (numpy), a random integer
is picked based on its state to seed the C++ code.
If None, default seeds in C++ code are used.</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code>, optional <code>(default=-1)</code></dt>
<dd>Number of parallel threads.</dd>
<dt><strong><code>silent</code></strong> :&ensp;<code>bool</code>, optional <code>(default=True)</code></dt>
<dd>Whether to print messages while running boosting.</dd>
<dt><strong><code>importance_type</code></strong> :&ensp;<code>string</code>, optional <code>(default="split")</code></dt>
<dd>The type of feature importance to be filled into
<code>feature_importances_</code>.
If "split", result contains numbers of times the feature
is used in a model.
If "gain", result contains total gains of splits
which use the feature.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_sklgb(self, trn_params):
    &#34;&#34;&#34;Parse parameters in to build the sklearn lgb.
    http://lightgbm.readthedocs.io/en/latest/Parameters.html

    Args:
        trn_params (dict): params to build model obj.
        boosting_type : string, optional (default=&#34;gbdt&#34;)
            &#34;gbdt&#34;, traditional Gradient Boosting Decision Tree.
            &#34;dart&#34;, Dropouts meet Multiple Additive Regression Trees.
            &#34;goss&#34;, Gradient-based One-Side Sampling.
            &#34;rf&#34;, Random Forest.
        num_leaves : int, optional (default=31)
            Maximum tree leaves for base learners.
        max_depth : int, optional (default=-1)
            Maximum tree depth for base learners, &lt;=0 means no limit.
        learning_rate : float, optional (default=0.1)
            Boosting learning rate.
            You can use `callbacks` parameter of `fit` method
                to shrink/adapt learning rate
                in training using `reset_parameter` callback.
            Note, that this will ignore the `learning_rate`
                argument in training.
        n_estimators : int, optional (default=100)
            Number of boosted trees to fit.
        subsample_for_bin : int, optional (default=200000)
            Number of samples for constructing bins.
        objective : string, callable or None, optional (default=None)
            Specify the learning task and the corresponding
                learning objective or a custom objective function
                to be used (see note below).
            Default: &#34;regression&#34; for LGBMRegressor, &#34;binary&#34; or
                &#34;multiclass&#34; for LGBMClassifier,
                &#34;lambdarank&#34; for LGBMRanker.
        class_weight : dict, &#34;balanced&#34; or None, optional (default=None)
            Weights associated with classes
                in the form `{class_label: weight}`.
            Use this parameter only for multi-class classification task;
                for binary classification task you may use `is_unbalance`
                or `scale_pos_weight` parameters.
            Note, that the usage of all these parameters will result in
                poor estimates of the individual class probabilities.
            You may want to consider performing probability calibration
            (https://scikit-learn.org/stable/modules/calibration.html)
                of your model.
            The &#34;balanced&#34; mode uses the values of y
                to automatically adjust weights
                inversely proportional to class frequencies
                in the input data
                as `n_samples / (n_classes * np.bincount(y))`.
            If None, all classes are supposed to have weight one.
            Note, that these weights will be multiplied with
                `sample_weight` (passed through the `fit` method)
                if `sample_weight` is specified.
        min_split_gain : float, optional (default=0.)
            Minimum loss reduction required to make a further partition on
                a leaf node of the tree.
        min_child_weight : float, optional (default=1e-3)
            Minimum sum of instance weight (hessian)
            needed in a child (leaf).
        min_child_samples : int, optional (default=20)
            Minimum number of data needed in a child (leaf).
        subsample : float, optional (default=1.)
            Subsample ratio of the training instance.
        subsample_freq : int, optional (default=0)
            Frequence of subsample, &lt;=0 means no enable.
        colsample_bytree : float, optional (default=1.)
            Subsample ratio of columns when constructing each tree.
        reg_alpha : float, optional (default=0.)
            L1 regularization term on weights.
        reg_lambda : float, optional (default=0.)
            L2 regularization term on weights.
        random_state : int, RandomState object or None,
            optional (default=None) Random number seed.
            If int, this number is used to seed the C++ code.
            If RandomState object (numpy), a random integer
                is picked based on its state to seed the C++ code.
            If None, default seeds in C++ code are used.
        n_jobs : int, optional (default=-1)
            Number of parallel threads.
        silent : bool, optional (default=True)
            Whether to print messages while running boosting.
        importance_type : string, optional (default=&#34;split&#34;)
            The type of feature importance to be filled into
                `feature_importances_`.
            If &#34;split&#34;, result contains numbers of times the feature
                is used in a model.
            If &#34;gain&#34;, result contains total gains of splits
                which use the feature.
    &#34;&#34;&#34;

    self.sk_api = True
    self.trn_params = trn_params</code></pre>
</details>
</dd>
<dt id="squirrelfriends.squirrel.lgbSquirrel.kfold_cv_lgb_train"><code class="name flex">
<span>def <span class="ident">kfold_cv_lgb_train</span></span>(<span>self, X, y, n_splits, seed)</span>
</code></dt>
<dd>
<div class="desc"><p>Train lightGBM by Kfold cross validation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>training data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>test data.</dd>
<dt><strong><code>n_splits</code></strong> :&ensp;<code>int</code></dt>
<dd>number of splits in Kflod().</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>random seed in Kfold().</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt>models (list of :obj:): list of models for each cv.</dt>
<dt><code>importances_tables (list</code> of <code>DataFrame):</code></dt>
<dd>list of feature importance tables for each cv.</dd>
<dt><code>eval_scores (list</code> of <code>float): list</code> of <code>score for each cv</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kfold_cv_lgb_train(self, X, y, n_splits, seed):
    &#34;&#34;&#34;Train lightGBM by Kfold cross validation.

    Args:
        X (DataFrame): training data.
        y (DataFrame): test data.
        n_splits (int): number of splits in Kflod().
        seed (int): random seed in Kfold().

    Returns:
        models (list of :obj:): list of models for each cv.
        importances_tables (list of DataFrame):
            list of feature importance tables for each cv.
        eval_scores (list of float): list of score for each cv
    &#34;&#34;&#34;

    if self.sk_api is None:
        raise Exception(&#34;Please call build function first.&#34;)

    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=seed)

    cv_results = []

    for fold, (ind_train, ind_test) in enumerate(kfold.split(X, y)):
        logging.info(&#34;cv: %d out of %d&#34; % (fold + 1, n_splits))

        X_train, X_test = X.loc[ind_train, :], X.loc[ind_test, :]
        y_train, y_test = y[ind_train], y[ind_test]
        data = (X_train, X_test, y_train, y_test)

        cv_results.append(self.train(data=data))

    eval_scores = [result[&#34;eval_score&#34;] for result in cv_results]

    if None not in eval_scores:
        logging.info(&#34;average score is %f, std is %f&#34; %
                     (np.mean(eval_scores), np.std(eval_scores)))

    importances_lst = [result[&#34;importances&#34;] for result in cv_results]
    self.feature_importances_ = compute_cv_feature_importances(
        *importances_lst)

    return cv_results</code></pre>
</details>
</dd>
<dt id="squirrelfriends.squirrel.lgbSquirrel.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, data)</span>
</code></dt>
<dd>
<div class="desc"><p>Train the model.</p>
<h2 id="args">Args</h2>
<p>data ((tuple of DataFrame)): training and testing data.</p>
<h2 id="returns">Returns</h2>
<p>model_result (dict): result of trained model.
- model (:obj:): trained model,
- importances (DataFrame): feature importance table,
- eval_score (float): score of model using <code>eval_func</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, data):
    &#34;&#34;&#34;Train the model.

    Args:
        data ((tuple of DataFrame)): training and testing data.

    Returns:
        model_result (dict): result of trained model.
            - model (:obj:): trained model,
            - importances (DataFrame): feature importance table,
            - eval_score (float): score of model using `eval_func`.
    &#34;&#34;&#34;

    if self.sk_api is None:
        raise Exception(&#34;Please call build function first.&#34;)

    if self.sk_api:
        model_result = self._train_sklgb(data)
    else:
        model_result = self._train_lgb(data)

    self.feature_importances_ = model_result[&#34;importances&#34;]

    return model_result</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="squirrelfriends" href="../index.html">squirrelfriends</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="squirrelfriends.squirrel.search_cv" href="#squirrelfriends.squirrel.search_cv">search_cv</a></code></li>
<li><code><a title="squirrelfriends.squirrel.smote_sampling" href="#squirrelfriends.squirrel.smote_sampling">smote_sampling</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="squirrelfriends.squirrel.lgbSquirrel" href="#squirrelfriends.squirrel.lgbSquirrel">lgbSquirrel</a></code></h4>
<ul class="">
<li><code><a title="squirrelfriends.squirrel.lgbSquirrel.build_lgb" href="#squirrelfriends.squirrel.lgbSquirrel.build_lgb">build_lgb</a></code></li>
<li><code><a title="squirrelfriends.squirrel.lgbSquirrel.build_sklgb" href="#squirrelfriends.squirrel.lgbSquirrel.build_sklgb">build_sklgb</a></code></li>
<li><code><a title="squirrelfriends.squirrel.lgbSquirrel.kfold_cv_lgb_train" href="#squirrelfriends.squirrel.lgbSquirrel.kfold_cv_lgb_train">kfold_cv_lgb_train</a></code></li>
<li><code><a title="squirrelfriends.squirrel.lgbSquirrel.train" href="#squirrelfriends.squirrel.lgbSquirrel.train">train</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>